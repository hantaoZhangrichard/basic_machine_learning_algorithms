{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg import Agent\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', g=9.81)\n",
    "agent = Agent(alpha=0.000025, beta=0.00025, input_dims=[3], tau=0.001, env=env,\n",
    "              batch_size=64,  layer1_size=400, layer2_size=300, n_actions=1)\n",
    "\n",
    "#agent.load_models()\n",
    "np.random.seed(0)\n",
    "\n",
    "score_history = []\n",
    "for i in range(500):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        act = agent.choose_action(obs)\n",
    "        new_state, reward, done, info = env.step(act)\n",
    "        agent.remember(obs, act, reward, new_state, int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        #env.render()\n",
    "    score_history.append(score)\n",
    "\n",
    "    #if i % 25 == 0:\n",
    "    #    agent.save_models()\n",
    "\n",
    "    print('episode ', i, 'score %.2f' % score,\n",
    "          'trailing 100 games avg %.3f' % np.mean(score_history[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_avg = np.zeros(len(score_history))\n",
    "for i in range(len(running_avg)):\n",
    "    running_avg[i] = np.mean(score_history[max(0, i-20):(i+1)])\n",
    "plt.plot(range(len(score_history)), running_avg)\n",
    "plt.title('Running average of previous 100 episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -116.89 trailing 100 games avg -116.893\n",
      "episode  1 score -98.71 trailing 100 games avg -107.800\n",
      "episode  2 score -125.01 trailing 100 games avg -113.536\n",
      "episode  3 score -122.28 trailing 100 games avg -115.722\n",
      "episode  4 score -102.19 trailing 100 games avg -113.015\n",
      "episode  5 score -112.27 trailing 100 games avg -112.890\n",
      "episode  6 score -122.30 trailing 100 games avg -114.234\n",
      "episode  7 score -104.31 trailing 100 games avg -112.993\n",
      "episode  8 score -97.31 trailing 100 games avg -111.251\n",
      "episode  9 score -98.35 trailing 100 games avg -109.961\n",
      "episode  10 score -128.76 trailing 100 games avg -111.670\n",
      "episode  11 score -97.77 trailing 100 games avg -110.512\n",
      "episode  12 score -111.92 trailing 100 games avg -110.620\n",
      "episode  13 score -102.80 trailing 100 games avg -110.062\n",
      "episode  14 score -101.68 trailing 100 games avg -109.503\n",
      "episode  15 score -97.95 trailing 100 games avg -108.781\n",
      "episode  16 score -66.87 trailing 100 games avg -106.315\n",
      "episode  17 score -109.40 trailing 100 games avg -106.486\n",
      "episode  18 score -107.75 trailing 100 games avg -106.553\n",
      "episode  19 score -51.85 trailing 100 games avg -103.818\n",
      "episode  20 score -106.67 trailing 100 games avg -103.954\n",
      "episode  21 score -108.69 trailing 100 games avg -104.169\n",
      "episode  22 score -113.55 trailing 100 games avg -104.577\n",
      "episode  23 score -106.03 trailing 100 games avg -104.637\n",
      "episode  24 score -119.66 trailing 100 games avg -105.238\n",
      "episode  25 score -103.51 trailing 100 games avg -105.172\n",
      "episode  26 score -70.12 trailing 100 games avg -103.874\n",
      "episode  27 score -110.13 trailing 100 games avg -104.097\n",
      "episode  28 score -58.50 trailing 100 games avg -102.525\n",
      "episode  29 score -99.59 trailing 100 games avg -102.427\n",
      "episode  30 score -121.65 trailing 100 games avg -103.047\n",
      "episode  31 score -101.55 trailing 100 games avg -103.000\n",
      "episode  32 score -98.56 trailing 100 games avg -102.866\n",
      "episode  33 score -97.34 trailing 100 games avg -102.703\n",
      "episode  34 score -97.11 trailing 100 games avg -102.544\n",
      "episode  35 score -98.51 trailing 100 games avg -102.432\n",
      "episode  36 score -97.71 trailing 100 games avg -102.304\n",
      "episode  37 score -97.96 trailing 100 games avg -102.190\n",
      "episode  38 score -96.05 trailing 100 games avg -102.032\n",
      "episode  39 score -96.82 trailing 100 games avg -101.902\n",
      "episode  40 score -102.60 trailing 100 games avg -101.919\n",
      "episode  41 score -97.69 trailing 100 games avg -101.818\n",
      "episode  42 score -124.38 trailing 100 games avg -102.343\n",
      "episode  43 score -99.16 trailing 100 games avg -102.271\n",
      "episode  44 score -100.23 trailing 100 games avg -102.225\n",
      "episode  45 score -98.57 trailing 100 games avg -102.146\n",
      "episode  46 score -98.82 trailing 100 games avg -102.075\n",
      "episode  47 score -99.54 trailing 100 games avg -102.022\n",
      "episode  48 score -98.34 trailing 100 games avg -101.947\n",
      "episode  49 score -100.95 trailing 100 games avg -101.927\n",
      "episode  50 score -99.35 trailing 100 games avg -101.877\n",
      "episode  51 score -101.50 trailing 100 games avg -101.869\n",
      "episode  52 score -98.02 trailing 100 games avg -101.797\n",
      "episode  53 score -100.78 trailing 100 games avg -101.778\n",
      "episode  54 score -100.20 trailing 100 games avg -101.749\n",
      "episode  55 score -100.87 trailing 100 games avg -101.734\n",
      "episode  56 score -97.39 trailing 100 games avg -101.657\n",
      "episode  57 score -70.68 trailing 100 games avg -101.123\n",
      "episode  58 score -102.33 trailing 100 games avg -101.144\n",
      "episode  59 score -100.27 trailing 100 games avg -101.129\n",
      "episode  60 score -100.39 trailing 100 games avg -101.117\n",
      "episode  61 score -98.02 trailing 100 games avg -101.067\n",
      "episode  62 score -107.96 trailing 100 games avg -101.177\n",
      "episode  63 score -95.90 trailing 100 games avg -101.094\n",
      "episode  64 score -97.83 trailing 100 games avg -101.044\n",
      "episode  65 score -101.82 trailing 100 games avg -101.056\n",
      "episode  66 score -101.89 trailing 100 games avg -101.068\n",
      "episode  67 score -100.83 trailing 100 games avg -101.065\n",
      "episode  68 score -98.37 trailing 100 games avg -101.026\n",
      "episode  69 score -99.54 trailing 100 games avg -101.004\n",
      "episode  70 score -104.04 trailing 100 games avg -101.047\n",
      "episode  71 score -99.97 trailing 100 games avg -101.032\n",
      "episode  72 score -97.99 trailing 100 games avg -100.990\n",
      "episode  73 score -121.59 trailing 100 games avg -101.269\n",
      "episode  74 score -101.84 trailing 100 games avg -101.277\n",
      "episode  75 score -102.32 trailing 100 games avg -101.290\n",
      "episode  76 score -102.06 trailing 100 games avg -101.300\n",
      "episode  77 score -97.78 trailing 100 games avg -101.255\n",
      "episode  78 score -91.29 trailing 100 games avg -101.129\n",
      "episode  79 score -58.24 trailing 100 games avg -100.593\n",
      "episode  80 score -119.22 trailing 100 games avg -100.823\n",
      "episode  81 score -104.53 trailing 100 games avg -100.868\n",
      "episode  82 score -153.69 trailing 100 games avg -101.505\n",
      "episode  83 score -84.69 trailing 100 games avg -101.304\n",
      "episode  84 score -102.13 trailing 100 games avg -101.314\n",
      "episode  85 score -102.34 trailing 100 games avg -101.326\n",
      "episode  86 score -98.06 trailing 100 games avg -101.288\n",
      "episode  87 score -98.38 trailing 100 games avg -101.255\n",
      "episode  88 score -98.93 trailing 100 games avg -101.229\n",
      "episode  89 score -98.11 trailing 100 games avg -101.195\n",
      "episode  90 score -130.16 trailing 100 games avg -101.513\n",
      "episode  91 score -96.28 trailing 100 games avg -101.456\n",
      "episode  92 score -148.46 trailing 100 games avg -101.961\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m new_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(act)\n\u001b[1;32m     16\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(obs, act, reward, new_state, \u001b[38;5;28mint\u001b[39m(done))\n\u001b[0;32m---> 17\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     19\u001b[0m obs \u001b[38;5;241m=\u001b[39m new_state\n",
      "File \u001b[0;32m~/Desktop/basic_machine_learning_algorithms/reinforcement_learning/ddpg/ddpg.py:258\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mforward(state, mu)\n\u001b[1;32m    257\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mmean(actor_loss)\n\u001b[0;32m--> 258\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_network_parameters()\n",
      "File \u001b[0;32m~/anaconda3/envs/bending_parameter/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bending_parameter/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "agent = Agent(alpha=0.000025, beta=0.00025, input_dims=[24], tau=0.001, env=env,\n",
    "              batch_size=64,  layer1_size=400, layer2_size=300, n_actions=4, max_action=1)\n",
    "\n",
    "#agent.load_models()\n",
    "np.random.seed(0)\n",
    "\n",
    "score_history = []\n",
    "for i in range(500):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        act = agent.choose_action(obs)\n",
    "        new_state, reward, done, info = env.step(act)\n",
    "        agent.remember(obs, act, reward, new_state, int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        #env.render()\n",
    "    score_history.append(score)\n",
    "\n",
    "    #if i % 25 == 0:\n",
    "    #    agent.save_models()\n",
    "\n",
    "    print('episode ', i, 'score %.2f' % score,\n",
    "          'trailing 100 games avg %.3f' % np.mean(score_history[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_avg = np.zeros(len(score_history))\n",
    "for i in range(len(running_avg)):\n",
    "    running_avg[i] = np.mean(score_history[max(0, i-20):(i+1)])\n",
    "plt.plot(range(len(score_history)), running_avg)\n",
    "plt.title('Running average of previous 100 episodes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bending_parameter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
